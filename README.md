# XGBoost_StockPrediction

  XGBoost is an efficient and popular gradient boosted tree algorithm which is implemented in an open-source manner. Gradient boosting is a supervised learning algorithm.  XGBoost attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models.
  XGBoost is an ensemble learning method.  Ensemble learning offers a systematic solution that combines the predictive power of multiple learners. The resultant is a single model that has the aggregated output from several models. The base learners that form the ensemble could be either from the same learning algorithm or different learning algorithms. The widely used ensemble learners are bagging and boosting. The most predominant usage of XGBoost has been decision trees followed by statistical models.
  When using gradient boosting for regression, each regression tree maps a data point to one of its leafs which has a continuous score and, the weak learners are regression trees. XGBoost minimizes a regularized objective function that combines a convex loss function and a penalty term for model complexity. The training proceeds iteratively, adding new trees that predict the residuals of prior trees that are then combined with previous trees to make the final prediction. It's called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.

  The algorithm involved and the implementation details of the XGBoost algorithm are discussed below in detail. The implementation is done in python using anaconda framework.
  
Bagging:

  The decision trees exhibit highly variable behavior, though decision trees are one of the most easily interpretable models. Consider a single training dataset that will be randomly split into two parts. Each part will train a decision tree to obtain two models. When it fits both these models, they will produce different results. Decision trees are said to be linked with high variance due to this behavior. Bagging or boosting aggregation helps to decrease the variance in any learner. Several decision trees which are created in parallel, form the base learners of the bagging technique. Data sampled with replacement is given to these learners for training. The final prediction output is the mean output from all the learners.

Boosting:

  In boosting, all the trees are built sequentially, such that each succeeding tree intentions to reduce the errors of the preceding tree. Each tree learns from its antecedents and updates the residual errors. Hence, the tree all the succeeding trees will learn from an updated version of the residuals. The base learners whose bias is very high are weak learners, and the predictive power is just a little better than accidental guessing. Each of these weak learners contributes vital information for prediction, resulting in the production of a strong learner by combining these weak learners. The last and final strong learner has low bias and variance.
In stark contrast to bagging techniques like Random Forest, where trees are grown to their maximum extent, the boosting method in XGBoost makes use of trees with fewer splits. Such small trees are highly interpretable because the depth of the tree is very less. Parameters like the number of iterations or trees, the learning rate of gradient boosting, and the tree depth, could be optimally selected through validation techniques like k-fold cross-validation. Overfitting is caused by having a large number of trees. So, it is necessary to choose the stopping criteria for boosting carefully.
